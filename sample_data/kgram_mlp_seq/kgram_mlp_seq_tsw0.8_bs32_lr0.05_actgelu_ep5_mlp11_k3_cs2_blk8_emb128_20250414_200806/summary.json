{
  "model_type": "kgram_mlp_seq",
  "inner_layers": 11,
  "k": 3,
  "chunk_size": 2,
  "block_size": 8,
  "embed_size": 128,
  "activation": "gelu",
  "learning_rate": 0.05,
  "batch_size": 32,
  "tinystories_weight": "0.8",
  "num_epochs": 5,
  "avg_loss": 3.778291845321655,
  "val_loss": 3.7571806302146307,
  "perplexity": 43.741261045572415,
  "token_accuracy": 0.1830357164144516,
  "grad_norm": 0.1304871539269749,
  "grad_norm_preclip": 0.18167274799235472,
  "grad_norm_postclip": 0.18167274799235472,
  "max_param_grad": 0.05405131690204144,
  "weight_norm": 2878.053495768718,
  "epoch": 5
}