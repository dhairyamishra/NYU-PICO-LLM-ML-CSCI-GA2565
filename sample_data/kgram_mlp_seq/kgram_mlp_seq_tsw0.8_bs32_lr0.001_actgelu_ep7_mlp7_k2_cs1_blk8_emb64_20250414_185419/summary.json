{
  "model_type": "kgram_mlp_seq",
  "inner_layers": 7,
  "k": 2,
  "chunk_size": 1,
  "block_size": 8,
  "embed_size": 64,
  "activation": "gelu",
  "learning_rate": 0.001,
  "batch_size": 32,
  "tinystories_weight": "0.8",
  "num_epochs": 7,
  "avg_loss": 4.427496147155762,
  "val_loss": 3.93703826268514,
  "perplexity": 83.72152786860849,
  "token_accuracy": 0.7276785969734192,
  "grad_norm": 0.999999689841391,
  "grad_norm_preclip": 2.576225198266543,
  "grad_norm_postclip": 0.9999996571670128,
  "max_param_grad": 0.3630534201860428,
  "weight_norm": 1801.3880273413909,
  "epoch": 7
}