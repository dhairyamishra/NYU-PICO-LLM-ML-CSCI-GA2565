{
  "model_type": "kgram_mlp_seq",
  "inner_layers": 5,
  "k": 3,
  "chunk_size": 3,
  "block_size": 16,
  "embed_size": 128,
  "activation": "gelu",
  "learning_rate": 0.001,
  "batch_size": 256,
  "tinystories_weight": "0.8",
  "num_epochs": 10,
  "avg_loss": 2.2450668811798096,
  "val_loss": 2.1723254919052124,
  "perplexity": 9.441046963705057,
  "token_accuracy": 0.5906250476837158,
  "grad_norm": 0.49980046637265646,
  "grad_norm_preclip": 0.5200950055439549,
  "grad_norm_postclip": 0.5200950055439549,
  "max_param_grad": 0.01723632337525487,
  "weight_norm": 2545.5483675779674,
  "epoch": 10
}