{
  "model_type": "kgram_mlp_seq",
  "inner_layers": 7,
  "k": 1,
  "chunk_size": 1,
  "block_size": 16,
  "embed_size": 128,
  "activation": "relu",
  "learning_rate": 0.001,
  "batch_size": 32,
  "tinystories_weight": "0.8",
  "num_epochs": 5,
  "avg_loss": 3.6559369802474975,
  "val_loss": 3.4565206217387368,
  "perplexity": 38.703768797695346,
  "token_accuracy": 0.4520833492279053,
  "grad_norm": 0.999999330680576,
  "grad_norm_preclip": 1.4477678840932973,
  "grad_norm_postclip": 0.999999343453865,
  "max_param_grad": 0.05796325840055942,
  "weight_norm": 2541.9514624747594,
  "epoch": 5
}